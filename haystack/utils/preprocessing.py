from distutils.command.clean import clean
from typing import Callable, Dict, List, Optional

import re
import logging
from pathlib import Path

from haystack.nodes.file_converter import BaseConverter, DocxToTextConverter, PDFToTextConverter, TextConverter
from haystack.schema import Document
from haystack.nodes.preprocessor import PreProcessor

logger = logging.getLogger(__name__)


def convert_files_to_docs(
    dir_path: str,
    clean_func: Optional[Callable] = None,
    split_paragraphs: bool = False,
    encoding: Optional[str] = None,
    preprocessor:Optional[PreProcessor] = PreProcessor(),     
    allowed_suffixes: Optional[List[str]] = [".pdf", ".txt", ".docx"],    
    use_tika: Optional[bool] = False,
    id_hash_keys: Optional[List[str]] = None,
) -> List[Document]:
    """
    Convert all files(.txt, .pdf, .docx) in the sub-directories of the given path to Documents that can be written to a
    Document Store.

    :param dir_path: The path of the directory containing the Files.
    :param clean_func: A custom cleaning function that gets applied to each Document (input: str, output: str).
    :param split_paragraphs: Whether to split text by paragraph.
    :param encoding: Character encoding to use when converting pdf documents.
    :param preprocessor: Preprocessor to use when converting pdf documents.
    :param allowed_suffixes: A list of file suffixes that should be considered for conversion.
    :param use_tika: Whether to use Tika to convert files to text. If set to True, the file converter will use Tika to convert any file types that are included in allowed_suffixes
    :param id_hash_keys: A list of Document attribute names from which the Document ID should be hashed from.
            Useful for generating unique IDs even if the Document contents are identical.
            To ensure you don't have duplicate Documents in your Document Store if texts are
            not unique, you can modify the metadata and pass [`"content"`, `"meta"`] to this field.
            If you do this, the Document ID will be generated by using the content and the defined metadata.
    """
    
    file_paths = [p for p in Path(dir_path).glob("**/*")]
    suffix2converter: Dict[str, BaseConverter] = {}

    suffix2paths: Dict[str, List[Path]] = {}
    for path in file_paths:
        file_suffix = path.suffix.lower()
        if file_suffix in allowed_suffixes:
            if file_suffix not in suffix2paths:
                suffix2paths[file_suffix] = []
            suffix2paths[file_suffix].append(path)
        elif not path.is_dir():
            logger.warning(
                "Skipped file {0} as type {1} is not supported here. "
                "See haystack.file_converter for support of more file types".format(path, file_suffix)
            )

    # set up converters for each file type
    if use_tika:
        try:
            from haystack.nodes.file_converter import TikaConverter
        except Exception as ex:
            logger.error("Tika not installed. Please install tika and try again. Error: {}".format(ex))
            raise ex
        converter = TikaConverter()
        xmlContent: Dict[str, bool] = {}
        # Apply one instance of TikaConverter for all file types that are listed in allowed_suffixes
        for file_suffix in suffix2paths.keys():
            suffix2converter[file_suffix] = converter
            # Add file types that should use Tika xmlContent. Some, like epub, should not 
            if file_suffix in ['.pdf', '.docx', '.txt']:            
                xmlContent[file_suffix] = True
            else: 
                xmlContent[file_suffix] = False
    else:
        for file_suffix in suffix2paths.keys():
            # No need to initialize converter if file type not present in allowed_suffixes
            if file_suffix == ".pdf":
                suffix2converter[file_suffix] = PDFToTextConverter()
            if file_suffix == ".txt":
                suffix2converter[file_suffix] = TextConverter()
            if file_suffix == ".docx":
                suffix2converter[file_suffix] = DocxToTextConverter()

    documents = []
    for suffix, paths in suffix2paths.items():
        for path in paths:
            logger.info("Converting {}".format(path))
            
            # If TikaConverter is used, pass xmlContent flag to converter
            
            if use_tika and suffix in xmlContent:                
                document = suffix2converter[suffix].convert(
                file_path=path, meta=None, encoding=encoding, id_hash_keys=id_hash_keys, xmlContent=xmlContent[suffix]
            )[0]
            else:
                
            # PDFToTextConverter, TextConverter, DocxToTextConverter and TikaConverter return a list containing a single Document
                document = suffix2converter[suffix].convert(
                    file_path=path, meta=None, encoding=encoding, id_hash_keys=id_hash_keys
                )[0]
            #add meta data with path name to document
            document.meta["name"] = path.name

            #optional cleaning function to be run prior to the preprocessor
            if clean_func:
                document.content = clean_func(document.content)

            #optional preprocessor to be run on the document that will clean and split the document into chunks based on the parameters that were passed in for the preprocessor
            if split_paragraphs:
                documents.extend(preprocessor.process(documents=[document]))
            else:
                documents.append(Document(content=document.content, meta=document.meta, id_hash_keys=id_hash_keys))
    return documents